# the P value is 0.01988 for the interaction term, which is significant based on the 0.05 criterion
# this implies that this term is necessary
# Going with:
# v1 (wrong): The P-value is small (less than 0.05). So, according to our criterion, we reject, which suggests that the interaction term is necessary
# v2: The P-value is larger than 0.05. So, according to our criterion, we would fail to reject, which suggests that the interaction terms may not be necessary.
# 4
lm(mpg ~ I(wt * 0.5) + factor(cyl), data = mtcars)
lm(mpg ~ I(wt * 1) + factor(cyl), data = mtcars)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit5<-lm(y~x)
hatvalues(fit5)[5]
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit6<-lm(y~x)
# save the hat values
hats<-hatvalues(fit6)
# find the highest hat value
highHatLoc<-which(hats==max(hats))
# find the df beta slope (2) for the highest hat value
dfbetas(fit6)[highHatLoc,2]
install.packages("gridExtra")
?step
install.packages("caret")
library(caret)
library(kernlab)
install.packages("kernlab")
library(kernlab)
data(spam)
inTrain<-createDataPartition(y=spam$type,p=0.75,list=F)
training<-spam[inTrain,]
testing<-spam[-inTrain,]
dim(training)
head(spam)
set.seed(32343)
modelFit<-train(type~.,data=training,method='glm')
install.packages("e1071")
modelFit<-train(type~.,data=training,method='glm')
modelFit
TP <- 99
FP <- 999
p <- TP/(TP + FP)
p
library(AppliedPredictiveModeling)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
head(diagnosis)
head(diagnosis,20)
head(predictors)
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[trainIndex,]
View(testing)
View(training)
adData = data.frame(diagnosis,predictors)
testIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[-testIndex,]
testing = adData[testIndex,]
View(testing)
View(training)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
names(training)
library(ggplot2)
q<-ggplot2(data=training,aes(x=Superplasticizer))
library(ggplot2)
q<-ggplot2(data=training,aes(x=Superplasticizer))
q<-ggplot(data=training,aes(x=Superplasticizer))
q+geom_histogram()
training1<-training
training1$Superplasticizer<-log(training$Superplasticizer)
q<-ggplot(data=training1,aes(x=Superplasticizer))
q+geom_histogram()
range(training$Superplasticizer)
range(training1$Superplasticizer)
training1$Superplasticizer<-log(training$Superplasticizer+1)
q<-ggplot(data=training1,aes(x=Superplasticizer))
q+geom_histogram()
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
names(predictors)
roi<-57:68
p<-predictors[,roi]
names(p)
preProc<-preprocess(p,method="pca")
preProc<-preProcess(p,method="pca")
test<-predict(preProc,training)
preProc<-preProcess(p,method="pca",pcaComp=2)
test<-predict(preProc,training)
str(p)
test<-predict(preProc,p)
head(preProc)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
names(predictors)
# note that the names that begin with "IL" are columns 57:68
roi<-57:68
p<-predictors[,roi]
prComp<-prcom(p)
prComp<-prcomp(p)
View(prComp)
plot(prComp$x[,1],prComp$x[,2])
prComp$rotation
pc<-prcomp(p)
pc[,1]
pc$x[,1]
sum(pc$x[,1])
pc$x %*% pc$x
pc$x %*% t(pc$x)
t(pc$x) %*% (pc$x)
confusionMatrix(testing)
preProc<-preProcess(p,method="pca",pcaComp=2)
test<-predict(preProc,p)
confusionMatrix(testing)
?preProcess
preProc<-preProcess(p,method="pca",thresh=0.8)
preProc$numComp
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
str(training)
exist(training$type)
training$type
training$diagnosis
modelFit1<-train(training$diagnosis~.,method="glm",data=training)
training[,1]
trainPC<-predict(preProc,training[,-1])
trainPC<-predict(preProc,log10(training[,-1]+1))
trainPC<-predict(preProc,training[,-1])
trainPC<-predict(preProc,predictors)
preProc<-preProcess(p,method="pca",thresh=0.8)
preProc$numComp
trainPC<-predict(preProc,predictors)
trainPC<-predict(preProc,training[,-1])
confusionMatrix(testing$diagnosis,predict(modelFit1,testing))
preProc
training[,-1]
dim(training[,-1])
dim(training)
trainPC<-predict(preProc,p)
?predict
trainPC
modelFit2<-train(training$diagnosis~.method="glm",data=trainPC)
modelFit2<-train(training$diagnosis~.,method="glm",data=trainPC)
testPC<-predict(preProc,testing[,-1])
testPC<-predict(preProc,testing[,roi])
trainPC<-predict(preProc,training[,roi])
testPC<-predict(preProc,testing[,roi])
preProc<-preProcess(p,method="pca",thresh=0.8)
preProc$numComp
trainPC<-predict(preProc,training[,roi])
testPC<-predict(preProc,testing[,roi])
trainPC<-predict(preProc,p)
modelFit2<-train(training$diagnosis~.,method="glm",data=trainPC)
modelFit2<-train(training$diagnosis~.,method="glm",data=p)
trainPC
dim(trainPC)
dim(training)
dim(p)
names(training)
training<-training[,roi]
training = adData[ inTrain,]
names(training)
roi<-58:69
training<-training[,roi]
training = adData[ inTrain,]
trainingIL<-training[,roi]
dim(trainingIL)
dim(training)
modelFit1<-train(training$diagnosis~.,method="glm",data=trainingIL)
confusionMatrix(testing$diagnosis,predict(modelFit1,testing))
trainPC<-predict(preProc,trainingIL)
preProc<-preProcess(trainingIL,method="pca",thresh=0.8)
trainPC<-predict(preProc,trainingIL)
modelFit2<-train(training$diagnosis~.,method="glm",data=trainPC)
confusionMatrix(testing$diagnosis,predict(modelFit2,testing))
modelFit2<-train(training$diagnosis~.,method="glm",data=trainPC)
confusionMatrix(testing$diagnosis,predict(modelFit2,testing))
confusionMatrix(testing$diagnosis,predict(modelFit2,trainPC))
testPC<-predict(preProc,testing[,-1])
testPC<-predict(preProc,testing[,roi])
confusionMatrix(testing$diagnosis,predict(modelFit2,testPC))
a<-confusionMatrix(testing$diagnosis,predict(modelFit2,testPC))
a$Accuracy
str(a)
summary(a)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
data(segmentationOriginal)
library(caret)
names(segmentationOriginal)
inTrain<-createDataPartition(y=segmentationOriginal$Case,p=0.7,list=FALSE)
inTrain<-createDataPartition(y=segmentationOriginal$Case,p=0.7,list=FALSE)
training<-segmentationOriginal[inTrain,]
testing<-segmentationOriginal[-inTrain,]
set.seed(125)
?train
?rpart
mod1<-train(Case~.,method="rpart",data=training)
print(mod1$finalModel)
plot(mod1$finalModel,uniform=TRUE,main="Classification Tree")
text(modFit$finalModel,use.n=TRUE,all=TRUE,cex=0.8)
text(mod1$finalModel,use.n=TRUE,all=TRUE,cex=0.8)
head(training)
View(training)
head(training$TotalIntench2)
head(training[,75:119])
summary(training$TotalIntench2)
summary(training$TotalIntenCh2)
sum(training$TotalIntenCh2==23000)
a0<-training[1,]
a<-a0
a$TotalIntenCh2<-23000
a<-a0
a$TotalIntenCh2<-23000
a$FiberWidthCh1
a$FiberWidthCh1<-10
a$PerimStatusCh1
a$PerimStatusCh1<-2
b$VarIntenCh4
b<-a0
b$VarIntenCh4
b$TotalIntenCh2<-50000
b$FiberWidthCh1<-10
b$VarIntenCh4<-100
c<-a0
c$TotalIntenCh2<-57000
c$FiberWidthCh1<-8
c$VarIntenCh4<-100
d$PerimStatusCh1
d<-a0
d$FiberWidthCh1<-8
d$VarIntenCh4<-100
d$PerimStatusCh1
d$PerimStatusCh1<-2
a0<-training[1,]
a<-a0
a$TotalIntenCh2<-23000
a$FiberWidthCh1<-10
a$PerimStatusCh1<-2
b<-a0
b$TotalIntenCh2<-50000
b$FiberWidthCh1<-10
b$VarIntenCh4<-100
c<-a0
c$TotalIntenCh2<-57000
c$FiberWidthCh1<-8
c$VarIntenCh4<-100
d<-a0
d$FiberWidthCh1<-8
d$VarIntenCh4<-100
d$PerimStatusCh1<-2
test1<-data.frame(a,b,c,d)
?data.frame
test1<-rbind(a,b,c,d)
predict(mod1,newdata=test1)
predict(mod1,newdata=training)
unique(training$Case)
predict(mod1,newdata=test1)
unique(training$Case)
inTrain<-segmentationOriginal$Case=="Train"
inTrain<-segmentationOriginal$Case=="Train"
training<-segmentationOriginal[inTrain,]
testing<-segmentationOriginal[-inTrain,]
# set seed to 125
set.seed(125)
mod1<-train(Case~.,method="rpart",data=training)
summary(training)
?segmentationOriginal
mod1<-train(Class~.,method="rpart",data=training)
install.packages("rattle")
fancyRpartPlot(mod1$finalModel)
library(rattle)
fancyRpartPlot(mod1$finalModel)
fancyRpartPlot(mod1$finalModel)
install.packages("rpart.plot")
fancyRpartPlot(mod1$finalModel)
library(AppliedPredictiveModeling)
install.packages("caret")
install.packages("AppliedPredictiveModeling")
install.packages("ggplot2")
install.packages("reshape2")
install.packages("rattle")
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
table(olive$Area)
data(olive)
head(olive)
summary(olive)
olive = olive[,-1]
summary(olive)
library(ggplot2)
qplot(Area,Palmitic,data=olive)
qplot(Palmitic,Palmitoleic,color=Area,data=olive)
modFit<-train(Area ~ ., method="rpart",data=olive)
library(caret)
modFit<-train(Area ~ ., method="rpart",data=olive)
library(rattle)
fancyRpartPlot(modFit$finalModel)
install.packages("rpart.plot")
fancyRpartPlot(modFit$finalModel)
str(olive)
unique(olive$Area)
newdata = as.data.frame(t(colMeans(olive)))
predict(modFit,newData=newData)
predict(modFit,newdata=newdata)
olive$Area<-factor(olive$Area)
modFit<-train(Area ~ ., method="rpart",data=olive)
install.packages("e1071")
modFit<-train(Area ~ ., method="rpart",data=olive)
fancyRpartPlot(modFit$finalModel)
predict(modFit,newdata=newdata)
newdata = as.data.frame(t(colMeans(olive)))
hist(olive$Area)
data(olive)
olive = olive[,-1]
hist(olive$Area)
install.packages("shiny")
install.packages("kable")
?kable
df<-data.frame(x=c(1,2,4,0),y=c(0.5,1,2,0))
fit<-with(df,lm(y~x))
summary(fit)
plot(df)
abline(fit)
fit
h3<-function(t0,t1,x){
t0 + t1*x
}
h3(0,0.5,df$x)
h3(0,0.5,4)
h3(-1,0.5,4)
shiny::runApp('Data Science Specialization/Data Science Specialization Courses Master/09_DevelopingDataProducts/shiny/inputApp')
shiny::runApp('Data Science Specialization/Data Science Specialization Courses Master/09_DevelopingDataProducts/shiny/inputApp')
install.packages("manipulate")
?manipulate
library(manipulate)
?manipulate
manipulate(hist(runif(x)),x = slider(0,10))
manipulate(hist(runif(x),breaks=10),x = slider(0,10))
manipulate(hist(runif(x),breaks=10),x = slider(1,10))
manipulate(hist(runif(x),breaks=10),x = slider(1,10000))
manipulate(hist(runif(x),breaks=10),x = slider(1,100000))
install.packages("rcharts")
install.packages("devtools")
install_github('rCharts', 'ramnathv')
require(devtools)
library(devtools)
install_github('rCharts', 'ramnathv')
install.packages("googleVis")
suppressPackageStartupMessages(library(googleVis))
M<-gvisMotionChart(Fruits,"Fruit","Year",options=list(width=600,height=400))
print(M,"chart")
print(M,"chart")
suppressPackageStartupMessages(library(googleVis))
M<-gvisMotionChart(Fruits,"Fruit","Year",options=list(width=600,height=400))
print(M,"chart")
print(M,"chart")
plot(M,"chart")
plot(M,"chart")
suppressPackageStartupMessages(library(googleVis))
M <- gvisMotionChart(Fruits, "Fruit", "Year",
options=list(width=600, height=400))
print(M,"chart")
suppressPackageStartupMessages(library(googleVis))
M<-gvisMotionChart(Fruits,"Fruit","Year",options=list(width=600,height=400))
plot(M,"chart")
print(M,"chart")
print(M,"chart")
source('~/.active-rstudio-document')
library(devtools)
devtools::install_github('rstudio/shinyapps')
shinyapps::setAccountInfo(name='tboats', token='EE181971D4814F4D40535AEAAF3784EC', secret='udn89qZTmdZhFXeSgX9M2Emc08ZUcQGNOlRfZri7')
install.packages("plotly")
install_github("ropensci/plotly")
install_github("ropensci/plotly")
install.packages("stringi")
install_github("ropensci/plotly")
library(devtools)
install_github("ropensci/plotly")
library(plotly)
set_credentials_file("tboats", "52ywe8h8t4")
py <- plotly()
trace0 <- list(
x = c(1, 2, 3, 4),
y = c(10, 15, 13, 17)
)
trace1 <- list(
x = c(1, 2, 3, 4),
y = c(16, 5, 11, 9)
)
response <- py$plotly(trace0, trace1, kwargs=list(filename="basic-line", fileopt="overwrite"))
response$url
library(manipulate)
myPlot <- function(s) {
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
manipulate(myPlot(s), s = slider(0, 2, step = 0.1))
library(rCharts)
data(airquality)
dTable(airquality, sPaginationType = "full_numbers")
install.packages("stringr")
devtools::install_github('muschellij2/slidify')
library(slidify)
setwd("E:/Dropbox/R/Data Science Specialization/Developing Data Products/courseProject/slidifyProject/SF home price analyzer")
slidify("index.Rmd")
library(knitr)
browseURL(url = index.html)
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
mn<-800000
hsd<-100000
alpha<-1.5
houseDist<-(rnorm(1000,mean=mn,sd=hsd)^alpha)/(mn^alpha)
hist(houseDist)
mn<-800000
hsd<-100000
alpha<-2.5
houseDist<-(rnorm(1000,mean=mn,sd=hsd)^alpha)/(mn^alpha)
hist(houseDist)
mn<-800000
hsd<-100000
alpha<-2.5
houseDist<-(rnorm(1000,mean=mn,sd=hsd)^alpha)/(mn^alpha)*mn
hist(houseDist)
library(ggplot2)
qplot(x=houseDist,geom="histogram")
mn<-800000
hsd<-100000
alpha<-3
houseDist<-(rnorm(1000,mean=mn,sd=hsd)^alpha)/(mn^alpha)*mn
qplot(x=houseDist,geom="histogram")
hist(houseDist)
library(ggplot2)
mn<-800000
hsd<-100000
alpha<-3
houseDist<-(rnorm(1000,mean=mn,sd=hsd)^alpha)/(mn^alpha)*mn
q<-qplot(x=houseDist,geom="histogram") + scale_x_continuous(labels = comma)
print(q)
alpha<-3
houseDist<-(rnorm(1000,mean=mn,sd=hsd)^alpha)/(mn^alpha)*mn
q<-qplot(x=houseDist,geom="histogram") + scale_x_continuous(labels = comma)
q<-qplot(x=houseDist,geom="histogram") + scale_x_continuous(labels = labels)
print(q)
q<-qplot(x=houseDist,geom="histogram") + scale_x_continuous()
print(q)
q<-qplot(x=houseDist,geom="histogram") + scale_x_continuous() + xlab("Home Price")
print(q)
q<-qplot(x=houseDist,geom="histogram") + scale_x_continuous() + xlab("Home Price") + ylab("Number of Homes")
print(q)
slidify("index.Rmd")
q<-qplot(x=houseDist,geom="histogram",binwidth=100000) + scale_x_continuous() + xlab("Home Price") + ylab("Number of Homes")
print(q)
houseDist<-(rnorm(1000,mean=mn,sd=hsd)^alpha)/(mn^alpha)*mn
q<-qplot(x=houseDist,geom="histogram",binwidth=100000) + scale_x_continuous() + xlab("Home Price") + ylab("Number of Homes")
print(q)
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
shiny::runApp('E:/Dropbox/R/Data Science Specialization/Developing Data Products/courseProject/shinyApp')
shiny::runApp('E:/Dropbox/R/Data Science Specialization/Developing Data Products/courseProject/shinyApp')
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
shiny::runApp('E:/Dropbox/R/Data Science Specialization/Developing Data Products/courseProject/shinyApp')
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
slidify("index.Rmd")
